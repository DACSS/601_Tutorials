---
title: 'Tutorial 7: Advanced Programming'
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
load("601-AdvancedProgramming.rdata")
tutorial_options(exercise.timelimit = 10)
```

## **Introduction**

In this tutorial, we will learn commands to do some more advanced programming tricks to help round out our skills in R. We'll learn about how to write our own functions, and about iteration in programming. 

Another big part of today's tutorial will be to learn more about piping functions in R with `%>%` and `group_by()`, something we've seen pop up in earlier tutorials. This is a crucial shortcut for using multiple commands in a sequence with the same data. It will make many other commands in the future much easier to use.

We will learn the following R Commands:

* `%>%`
* `group_by()`
* `function(){}`
* `for(){}`

We will use the following datasets:

* Post-Secondary Employment Outcomes: **pseo** 
* Worldwide Access to Electricity: **electricity**

## **Piping Basics**

Suppose we want to both select certain variables and combine them? Turns out that is possible using piping. Piping is a process that revolves around the code `%>%`, known as the pipe, which allows you to run multiple commands in succession and **pipe** the results from one command to the next command. 

In this tutorial we have learned to calculate different statistics with `summarize()` and `summarize_all()`. In the last tutorial we learned how to select specific columns with `select()`. Imagine you want to use these functions together to calculate the median values for the years in **electric** that begin with "201" (the current decade).

Without piping, it would look like this, with the select function embedded within the summarize_all function:

```{r, echo=TRUE}
#Find the median of every 201- year in electric (no piping)
summarize_all(select(electric, starts_with("201")), median, na.rm = TRUE)
```

This is a little messy- and using R like will get even more confusing going forward as we use more functions and need more and more parantheses in each line of code.

Below, we accomplish the same task by piping with `%>%`.

```{r, echo=TRUE}
#Find the median of every 201- year in electric (with piping)
electric %>%
  select(starts_with("201")) %>%
  summarize_all(median, na.rm = TRUE)
```

It's okay if this format is initially confusing. It looks fundamentally different than everything else we have encountered so far in R. Let's unpack what we just did step-by-step.

When piping in R, the first thing you do is write the name of the data you are looking at, followed by the pipe symbol, `%>%`.

```{r, eval=FALSE}
electric %>%
```

Then press enter to continue to the next line. R will automatically indent this line of code to signify it is modifying **electric** via the pipe. 

On this indented line, write the first function being applied to **electric**. Because of the first line, you can omit any reference to **electric** inside (`select(starts_with("201"))`, not `select(electric, starts_with("201"))`). At the end of the line, use another `%>%`. 

```{r, eval=FALSE}
electric %>%
  select(starts_with("201")) %>%
```

On the next line, you can write another function to be applied to **electric** after the first function. Piping works in a sequence, so in this example, `summarize_all()` is applied to the subset of **electric** that has already been specified with `select()`. The final line does not need a `%>%` at the end- this lets R know to finish the multi-line function you have assembled. 

```{r, eval=FALSE}
electric %>%
  select(starts_with("201")) %>%
  summarize_all(median, na.rm = TRUE)
```

That is the basic format of piping. It can contain as many lines as you want; just remember to keep using `%>%`. 

Piping is useful when we want to modify our original data in one or more ways (e.g., using `select()`, `group_by()`,`filter()`, or `mutate()`) and then calculate statistics for the modified data. It saves A LOT of time and energy in big R projects. As we learn more commands in later tutorials, we will oten use the piping command to strategically pipe different commands together. 

### Exercise 1

Use `select()`, `summarize_all()`, and `%>%` piping in the same format as shown above to find the median of **p25_earnings**, **p50_earnings**, and **p75_earnings** in **pseo**. 

```{r pipe, exercise=TRUE}
#Find the median of every earnings column in pseo (with piping)

```

```{r pipe-solution}
#Find the median of every earnings column in pseo (with piping)
pseo %>%
  select(ends_with("earnings")) %>%
  summarize_all(median, na.rm = TRUE)
```


## **Grouping with Summarize**

Now that we know about piping, let's consider other ways we might modify data prior to using `summarize()` or `summarize_all()`. There is a very useful function called `group_by()` which can be used with the summarize functions to find summary statistics for different groups within the same data. It sounds complicated, but it is very easy to use with piping.

Let's look at the **pseo** data for an example. The average earnings of college graduates are likely to be very different at different intervals of time after graduation. We might want to find individual averages for different groups of graduates based on their time out of school... which we can do with `group_by()`. 

`Group_by()` follows the easy format: `group_by(column)`. This separates data into groups based on the different values of that column. It is most effectively used as first function when piping. In the example below, `group_by()` is used to summarize the earnings columns of **pseo** in groups based on the column **year_postgrad**. 

```{r, echo=TRUE}
pseo %>%
  group_by(year_postgrad) %>%
  select(ends_with("earnings")) %>%
  summarize_all(median, na.rm = TRUE)
```

With this one extra line of code, now there are three rows of results, based on the three groups of graduate earnings: 1 year, 5 years, and 10 years after graduation. These grouped numbers show how average earnings are likely to climb in the decade after one graduates college. This is very useful- the more detailed and specific your descriptive statistics are, the better!

The order that you use `group_by()` when piping is very important. If incorrect, you will get an error. 

```{r, echo=TRUE, error=TRUE}
pseo %>%
  select(ends_with("earnings")) %>%
  group_by(year_postgrad) %>%
  summarize_all(median, na.rm = TRUE)
```

Above, `group_by()` is used in the line after `select()`. This returns an error because if you narrow down the data with `select()` first, there is no longer a **year_postgrad** column available for `group_by()` to make groups with. 

It is also important not to forget to use `select()` in the process. This reduces the data to only include its columns of numeric data, or whichever specific numeric columns you want to focus on.

```{r, echo=TRUE, error=TRUE}
pseo %>%
  group_by(year_postgrad) %>%
  summarize_all(median, na.rm = TRUE)
```

`Summarize_all()` will find the averages of every column in the data if it is not specified with `select()`. This is problematic with **pseo** and many other datasets because not every column has numeric data. An error results from attempting to find the median, or any other quantitative statistic, of nominal data.

With these rules in mind, using `group_by` with `summarize_all()` is extremely practical for finding and comparing statistics for different groups of the same data.

### Exercise 2

Summarize in groups according to the format shown above to find the median earnings of college graduates in **pseo** based on their major (column **ciptitle**).

```{r grouping, exercise=TRUE}
#Find the median earnings in pseo, grouped by ciptitle

```

```{r grouping-solution}
#Find the median earnings in pseo, grouped by ciptitle
pseo %>%
  group_by(ciptitle) %>%
  select(ends_with("earnings")) %>%
  summarize_all(median, na.rm = TRUE)
```


## **Writing Functions**

To this point, we have taken advantage of all of the functions that others have wrote to do work in R. However, we can write our own functions too. This is particularly useful when there is a task that might be repeated across different datasets, variables, or other settings.

As an example, consider the following. One thing we often want to do, particularly in machine learning approaches, is to standardize our variables onto a common scale. Typically, one standardizes a variable by subtracting the mean from each value, and dividing by the standard deviation. We can write a function to do that! Here's what it would look like:

```{r, echo=TRUE}
myScalingFunction <- function(x) {
  (x - mean(x, na.rm=T))/ sd(x, na.rm=T)
}

myScalingFunction(pseo$p50_earnings)
```

That's a lot of `NA`s! Any ideas as to why? If you guessed they are the missing observations in the original variable (e.g., `p50_earnings`), then you are correct! 

Note: there is a `scale()` function in R, of course. The idea here is just to demonstrate how we could build these ourselves!

## **Iteration Using Loops**

What if we wanted to do the same thing lots and lots of times? As an example, consider flipping a coin. We could write a function to flip the coin like so:

```{r, echo=TRUE}
flip <- function() {
  sample(c("T", "H"), 1)
}

flip()

```

Each time we type `flip()`, we can flip the coin. What if we wanted to do that 1,000 times? 10,000? 1,000,000? We can do that in R using loops, which allow us to repeat an action (or a function) as many times as we need to. We could, of course, just flip a coin that many times. Maybe we want to make the problem even more complex though. How about this: how many flips does it take, on average, to get three heads in a row? 

To do so, I run 1,000 quick experiments. For each, I "flip" the coin until I get three heads in a row.


```{r, echo=TRUE}
# initialize a vector to store the output
output <- vector("double", 1000)

# set up the for loop; we're going to 1,000 iterations. "i" indexes our observation.
for (i in 1:1000){
  # initialize flips and number of heads
  flips <- 0
  nheads <- 0
  
  # while loops repeat until the condition is satisfied. Here, that means until nheads increases to 3
  while (nheads < 3) {
    # if loops only execute if the condition is matched. Here, if I flip a head, I execute the code below, adding one to nheads. Else loops execute if the condition is not satisfied. Here, if I flip a tails, I reset nheads to 0.
    if (flip() == "H") {
      nheads <- nheads + 1  
    } else {
      nheads <- 0
    }
    # Add one to flips to count up how many flips it is taking.
    flips <- flips + 1
  }
  # If I made it here, I got three heads in a row! I add the number of flips to the output vector in the i-th position, then move to the next loop.
output[i] <- flips
}

ggplot() + 
  geom_histogram(aes(output), bins=50)

table(output)

```



Don't worry if the intuition of the above isn't immediately clear. It can take a bit to understand iterations and conditional statements in programming, especially if you don't have prior background in programming. Once you have started to use these, though, you will find them to be particularly powerful way to improve what you can achieve in R, and to simplify what might seem like a really complicated action. 

## **Conclusion & Glossary**

We've introduced the pipe (`%>%`) at numerous points, but have finally really explored how it can be used in our analyses (or pipelines). It should be clear now, as you think back over each of the steps we've taken to this point, how powerful R and the tidyverse are as an engine for data science.

Coming up in the next (and last!) tutorial... we will learn the basics of modeling in R. 

### Function Glossary

Here's a recap of the functions you have encountered so far:

**Tutorial 1: R Basics and Assigning Variables**

* Basic mathematics: `+`,`-`,`*`,`/`
* Logical operators: `==`,`>`,`<`,`>=`,`>=`
* Assign name: `<-`
* Create vector: `c()`


**Tutorial 2: Reading and Describing Data**

* Preview data: `head()`
* Get dimensions: `dim()`
* Get column names and numbers: `colnames()`
* Select specific columns: `select()`
  + `starts_with()`
  + `ends_with()`
  + `contains()`
* Get values of column: `table()`
* Get proportions of values of column: `prop.table()`
* Collapse multiple columns into new rows: `gather()`
* Turn row data into new columns: `spread()`

**Tutorial 3: Intro to Visualization**

* Make crosstabs: `xtabs()`
* Make graphs: `ggplot()`
  + Histogram: `geom_histogram()`
  + Density: geom_density()
  + Bar graph: `geom_bar()`
  + Scatterplot: `geom_point()`
  + Boxplot: `geom_boxplot()`
  + Violin plot: `geom_violin()`
  + Smooth line plot: `geom_smooth()`

**Tutorial 4: Advanced Visualization**

* Make crosstabs: `xtabs()`
* Make graphs: `ggplot()`
  + Divide plot into subplots: `facet_wrap()`
* Color observations: `aes(fill =)`


**Tutorial 5: Transforming Data**
* Select specific rows: `filter()`
  + More logical operators: `&`, `|`, `!`
* Reorder rows: `arrange()`
* Select first x rows: `slice()`
* Rename column: `rename()`
* Recode column values: `recode()`
* New column: `mutate()`
* Convert value to NA: `na_if()`
* Convert NA to value: `replace_na()`
* Recode with individual arguments: `case_when()`


**Tutorial 6: Data Structures**

* Summarize stats for single column: `summarize()`
* Summarize one stat for all columns: `summarize_all()`
 + Find mean: `mean()`
 + Find median: `median()`
 + Find lowest value: `min()`
 + Find highest value: `max()`
 + Find standard deviation: `sd()`
 + Find variance: `var()`
 + Find interquartile range: `IQR()`
* Find variable format info: `str()`

**Tutorial 7: Advanced Programming**
* Piping: `%>%`
* Grouping: `group_by()`
* Functions: `function(){}`
* Iteration: `for(){}`



