---
title: "Tutorial 9: Basics of Modeling"
runtime: shiny_prerendered
output: learnr::tutorial
---

```{r setup, include=FALSE}
library(learnr)
library(sjlabelled)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
load("601_ModelingBasics.rdata")
tutorial_options(exercise.timelimit = 10)
```

## **Introduction**

This tutorial introduces different bivariate descriptions and tests that can be used to explore the relationship between two variables. The choice of data description and statistical test depends on the characteristics and level of the two variables involved.

We will learn the following R Commands:

* `t.test()`
* `chisq.test()`
* `tapply()`
* `anova()`
* `lm()`
* `summary(lm())`
* `cor()`

And we will use the following datasets:

* 2016 American National Election Study: **anes**
* Post-Secondary Employment Outcomes: **pseo** 
* Example datasets:
  + Sleep drug experiment: **sleep**
  + Infertility data: **infert**
  + Old Faithful eruptions: **faithful**
  + Chicken weights: **chicks**
  + Speed and stopping distance of cars: **cars**

## **T-test**

A *t-test* can be used to test for a difference in mean between groups. A *t-test* is a good test when you have one binary variable that divides your subjects up into groups (e.g., boys and girls), and one interval or ratio variable that can be used to calculate an arithmetic mean. 

The `t.test()` function follows the format: `test(DV ~ IV, data)`, where **DV** is the dependent variable and **IV** is the independent variable. 

Here is a short example using an example dataset called **sleep**, in which hours of additional sleep (**extra**) are measured between subjects who take two different drugs (**group**).  The *t.test* is used to compare whether or not the drugs made a significant difference in patients' sleep.

```{r ttest, echo=TRUE}
#group: IV - which of 2 drug treatments was received
#extra: DV - additional hours of sleep after treatment
#conduct t.test(DV~IV, data)
t.test(extra ~ group, data = sleep)
```

Inspect the **p-value** in the output. **P-values** below 0.05 are considered statistically significant.

```{r quiz1, echo = FALSE}
quiz(
  question("Does drug 2 cause subjects to sleep more than drug 1?",
    answer("No"),
    answer("Yes, but the difference is just short of standard statistical significance", correct=TRUE),
    answer("Yes, and the difference is stasticially signficant"),
    answer("Can't tell")
  )
)
```

### Exercise 1

Below, a few variables are recoded from the **anes** dataset so that we can easily refer to them. **Party** represents respondents' political party, **business** represents their feeling (0-100) towards "Big Business", **liberals** represents their feeling towards "Liberals" and **rich** represents their feeling towards "Rich People". 

```{r, eval=TRUE}
#recode v162136x as Econ_Mobility and v162034a as Vote_16
anes<-anes%>%
  mutate(party = case_when(
         v162030 == 1 ~ "Democratic",
         v162030 == 2 ~ "Republican"
         )) %>%
  mutate(business = v162100) %>%
  mutate(liberals = v162097) %>%
  mutate(rich = v162105)
```

Using the **anes** dataset, run t-tests to see whether there is a difference in the mean feeling thermometer value towards big business, liberals, and rich people between Democratic and Republican respondents.

```{r ttEx, exercise=TRUE}
#conduct t.test(DV~IV, data)

```

```{r ttEx-solution}
#conduct t.test(DV~IV, data)
t.test(business~party, data=anes)
t.test(liberals~party, data=anes)
t.test(rich~party, data=anes)
```

## **Chi-squared test**

A *chi-squared test* is used to test for differences in the conditional distribution of one variable on another. It is used when both variables are categorical (or ordinal), and each has a fairly small number of categories. It is a good idea to visually inspect the cross-tab to identify which categories have deviations from expected values, in addition to looking at results of the statistical test.

The `chisq.test()` function follows the format: `chisq.test(data$IV,data$DV)`. 

The example below uses an dataset called **infert** in which the variable **induced** represents a subject's number of prior induced abortions and the variable **case** represents whether or not the subject is infertile.

```{r chisq, echo=TRUE}
#induced: IV number of prior induced abortions (0,1, 2+)
#case: DV is the subject infertile (1=yes, 0=no)

#inspect cross-tab: table
xtabs(~ induced + case, infert)
#inspect cross-tab with column proportions
prop.table(xtabs(~ induced + case, infert))
#calculate chisq.test
chisq.test(infert$induced,infert$case)
```

Inspect the **p-value** like you did with a t-test.

```{r quiz2, echo = FALSE}
quiz(
  question("Does a women's (in)fertility conditionally depend on the number of planned abortions she has had in the past?",
    answer("No", correct=TRUE),
    answer("Yes, but the difference is just short of standard statistical significance"),
    answer("Yes, and the difference is stasticially signficant"),
    answer("Can't tell")
  )
)
```


### Exercise 2

Some additional recoded variables from the **anes** dataset: **china** represents respondents' opinion on if China is a military threat, and **hcare** represents if respondents think there should be increased or decreased government spending to help people pay for health care.

```{r, eval=TRUE, echo=TRUE}
anes<-anes%>%
  mutate(china = case_when(
         v162159 == 1 ~ "Major threat",
         v162159 == 2 ~ "Minor threat",
         v162159 == 3 ~ "Not a threat"
         )) %>%
  mutate(hcare = case_when(
         v162193 == 1 ~ "Increase",
         v162193 == 2 ~ "Decrease"
         )) 
```

Using the **anes** data, analyze whether there is a significant difference in opinions on China between those who support increasing or decreasing healthcare spending.

```{r chisqEx, exercise=TRUE, exercise.lines=4}
#inspect crosstab

#calculate chisq.test

```

```{r chisqEx-solution}
#inspect crosstab
prop.table(xtabs(~ china + hcare, anes))
#calculate chisq.test
chisq.test(anes$china,anes$hcare)
```


## **Categorical Means, Anova and Regression**

A *categorical mean* or *group mean* is useful to calculate when one variable is interval/ratio, and the other variable is either nominal/categorical or ordinal (and more rarely, interval). In R, this typically means using a factor variable. This is useful when we have a treatment with more than two conditions. The R command `tapply()` can be used to easily calculate categorical group means. This function follows the format: `tapply(data$DV,data$catIV,mean)` (other stats could be substituted for mean, but that's what we're finding here) in which *catIV* stands for categorical independent variable.

This example data is called **chicks**, which contains chicken weights (**weight**) and the type of feed that each chicken eats (**feed**).

```{r groupmean, echo=TRUE}
#inspect table
table(select(chicks, feed))
#calculate group means of DV for categorical IV
tapply(chicks$weight,chicks$feed,mean)
```

**NOTE:** If you have any missing values in your data, the option "mean" will need the additional argument `na.rm=TRUE` in order to correctly calculate categorical means. See the example code below:

```{r tapplynote, results=FALSE}
#calculate group means of DV for categorical IV, excluding missing values
tapply(chicks$weight,chicks$feed,mean,na.rm=TRUE)
```

Just like a *t-test* tests for differences in the means of two groups, *anova* tests for differences in between the means of two or more groups (i.e., the categorical means calculated above.) Unlike a t-test, you must pass a bivariate linear model using `lm()` to the R function `anova()`. 

```{r anova, echo=TRUE}
#calculate anova using bivariate regression lm(DV~catIV)
anova(lm(chicks$weight~chicks$feed))
```

Note that the R syntax makes it clear that an anova is actually a joint test of significance for the underlying regression model. Inspecting this model allows us to assess which groups appear to be different from each other, although including categorical variables in regressions is tricky as the significance of any single factor level depends on which level is treated as the *baseline* or *omitted category*. Anova is usually a good sanity check for any regression model including any factor variable  as it isn't sensitive to the choice of baseline category.

We can inspect the model with the `summary()` function applied to `lm()`.

```{r catreg, echo=TRUE}
#estimate bivariate regression lm(DV~catIV)
summary(lm(chicks$weight~chicks$feed))
```

```{r quiz4, echo = FALSE}
quiz(
  question("Does chicken weight-gain depend on what food they are given?",
    answer("No"),
    answer("Yes, and casein and sunflower are significantly better than the other 4 foods"),
    answer("Yes, and casein and sunflower are significantly better than the other 3 foods, but not necessarily better than meatmeal", correct=TRUE),
    answer("Can't tell")
  )
)
```

### Exercise 3

Let's try using the **pseo** dataset to answer the question: Do college graduates with different degree levels earn more? As a reminder, **deglevl** represents degree level, and **p50_earnings** represents average annual earnings. 

The answer should be fairly intuitive, but you should investige this using all three techniques shown above: categorical group means, anova, and bivariate regression. Remember to put the dependent variable and the independent variable in the correct order, and to exclude missing values.

```{r catEx, exercise=TRUE, exercise.lines=6}
#calculate group means of DV for categorical IV

#calculate anova using bivariate regression lm(DV~catIV)

#estimate bivariate regression lm(DV~catIV)

```

```{r catEx-solution}
#calculate group means of DV for categorical IV
tapply(pseo$p50_earnings,pseo$deglevl,mean,na.rm=TRUE)
#calculate anova using bivariate regression lm(DV~catIV)
anova(lm(pseo$p50_earnings~pseo$deglevl))
#estimate bivariate regression lm(DV~catIV)
summary(lm(pseo$p50_earnings~pseo$deglevl))
```

## **Correlation and Regression**

A *correlation* between two variables is calculated when both variables are interval/ratio varibles. In general, correlations of over 0.20 are noteworthy in survey research, and even smaller correlations may be substantively and statistically signficiant when working with individual level data that is typically quite noisy. A **bivariate linear model** can be used to check for the statistical signficance of a relationship between two interval/ratio variables.

The `cor()` function follows the format `cor(data$variable,data$variable)` (variable order doesn't matter). To estimate the bivariate linear model, use `summary(lm(data$DV~data$IV))` like in the last section.

Our next random dataset is **cars**, which includes the speed of cars and the distances taken for them to stop (note: the data were recorded in the 1920s). 

```{r cortest, echo=TRUE}
#speed: speed of car
#dist: stopping distince of car (in feet)

#calculate the correlation coefficient
cor(cars$dist,cars$speed)
#estimate the bivariate linear model lm(DV~IV)
summary(lm(cars$dist~cars$speed))
```

**Note:** If there are any missing values in one or both variables, include the argument `use=parwise.complete.obs` in the *cor* command. Like so:

```{r, results=FALSE}
#calculate the correlation coefficient, excluding missing values
cor(cars$dist,cars$speed, use="pairwise.complete.obs")
```

```{r quiz5, echo = FALSE}
quiz(
  question("Does stopping distance depend on how fast the car is going?",
    answer("No"),
    answer("Yes, but the difference is just short of standard statistical significance"),
    answer("Yes, and the difference is stasticially signficant", correct=TRUE),
    answer("Can't tell")
  )
)
```

### Exercise 4

Do college graduates earn more after more years post-graduation? Investige this claim using the **p50_earnings** and **year_postgrad** variables in the **pseo** dataset, using both techniques describe above: correlation and bivariate regression. Remember to exclude missing values.

```{r corEx, exercise=TRUE, exercise.lines=4}
#calculate the correlation coefficient

#estimate the bivariate linear model lm(DV~IV)

```

```{r corEx-solution}
#calculate the correlation coefficient
cor(pseo$p50_earnings,pseo$year_postgrad, use="pairwise.complete.obs")
#estimate the bivariate linear model lm(DV~IV)
summary(lm(p50_earnings~year_postgrad,data=pseo))
```

## **Conclusion & Glossary**

You now know how to run a variety of bivariate statistical tests on different kinds of variables, as well as how to interpret the results to determine if bivariate relationships are significant. 

This is final R tutorial of DACSS 601 - Congratulations! You should now have a coherent understanding of how R works and a familiarity with tidyverse functions that are essential for data analysis. In the remaining homework assignment and your final project, we will put to use the tools we've learned about in these tutorials and the lectures. 

### Function Glossary

Here's a recap of the functions you have encountered so far:

**Tutorial 1: R Basics and Assigning Variables**

* Basic mathematics: `+`,`-`,`*`,`/`
* Logical operators: `==`,`>`,`<`,`>=`,`>=`
* Assign name: `<-`
* Create vector: `c()`

**Tutorial 2: Reading and Describing Data**

* Preview data: `head()`
* Get dimensions: `dim()`
* Get column names and numbers: `colnames()`
* Select specific columns: `select()`
  + `starts_with()`
  + `ends_with()`
  + `contains()`
* Get values of column: `table()`
* Get proportions of values of column: `prop.table()`
* Collapse multiple columns into new rows: `gather()`
* Turn row data into new columns: `spread()`

**Tutorial 3: Intro to Visualization**

* Make crosstabs: `xtabs()`
* Make graphs: `ggplot()`
  + Histogram: `geom_histogram()`
  + Density: geom_density()
  + Bar graph: `geom_bar()`
  + Scatterplot: `geom_point()`
  + Boxplot: `geom_boxplot()`
  + Violin plot: `geom_violin()`
  + Smooth line plot: `geom_smooth()`

**Tutorial 4: Advanced Visualization**

* Make crosstabs: `xtabs()`
* Make graphs: `ggplot()`
  + Divide plot into subplots: `facet_wrap()`
* Color observations: `aes(fill =)`


**Tutorial 5: Transforming Data**
* Select specific rows: `filter()`
  + More logical operators: `&`, `|`, `!`
* Reorder rows: `arrange()`
* Select first x rows: `slice()`
* Rename column: `rename()`
* Recode column values: `recode()`
* New column: `mutate()`
* Convert value to NA: `na_if()`
* Convert NA to value: `replace_na()`
* Recode with individual arguments: `case_when()`


**Tutorial 6: Data Structures**

* Summarize stats for single column: `summarize()`
* Summarize one stat for all columns: `summarize_all()`
 + Find mean: `mean()`
 + Find median: `median()`
 + Find lowest value: `min()`
 + Find highest value: `max()`
 + Find standard deviation: `sd()`
 + Find variance: `var()`
 + Find interquartile range: `IQR()`
* Find variable format info: `str()`

**Tutorial 7: Advanced Programming**
* Piping: `%>%`
* Grouping: `group_by()`
* Functions: `function(){}`
* Iteration: `for(){}`

**Tutorial 8: Modeling Basics**

* Run t-test: `t.test()`
* Run chi-squared test: `chisq.test()`
* Calculate group means: `tapply()`
* Run ANOVA test: `anova()`
  + Run linear model: `lm()`
  + Inspect linear model: `summary(lm())`
* Calculate correlation coefficient: `cor()`