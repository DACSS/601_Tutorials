---
title: "Tutorial 9: Vectorization"
runtime: shiny_prerendered
output: learnr::tutorial
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, message=FALSE}
library(learnr)
library(sjlabelled)
library(tidyverse)
library(nycflights13)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=here::here('9_Vectorization&Benchmarking/prep/files'))
#tutorial_options(exercise.timelimit = 10)
```

## **Introduction**
In the previous tutorial, you learned about using for loops to repeat an
operation in each value in a series of values. In this tutorial, we will
contrast for loops with an alternative technique, *vectorization*. You
will learn how to vectorize functions as well as compare the time it
takes to run functions in R with a technique called *benchmarking*.

We will learn the following R Commands:

â€¢ `microbenchmark()`
<!-- add all commands used in tutorial -->

And we will use the following datasets: 
The `flights` dataset from the R package `nycflights13`, which contains all flights out of New York City in 2013.

## **Benchmarking**
Before we learn how to compare for loops to vectorization, we need to learn about *benchmarking*. Benchmarking is used to measure the amount of time it takes to perform a computation in R. There are a number of ways to do so, but we will be using the R package `microbenchmark` [(link to CRAN page)](https://cran.r-project.org/web/packages/microbenchmark/).

### microbenchmark

To install `microbenchmark`, run the following code:
```{r installing microbenchmark, eval=F}
install.packages('microbenchmark')
```
  
Then to use it, load the library:  
```{r load microbenchmark library}
library(microbenchmark)
```

We can then use the `microbenchmark()` function to output timing results for a line of R code. Here, let's show how to measure the time it takes to get the mean of a numeric vector.

```{r benchmark the mean() function}
vec <- runif(100) #randomly sample 100 values from a uniform distribution
test_results <- microbenchmark(mean(vec))
```

`microbenchmark` works by running the code many times and calculating the runtime for each calculation. Each individual runtime is stored in `test_results`. Let's take a look at them here:  

```{r glimpse test results}
test_results
```
As you can see, there is some variability in runtime. This happens for a variety of reasons, but it doesn't particularly matter for us. We are interested in how long things \textbf{generally} take to run in R, so we'll compute some summary statistics. We can summarize the results using `summary(test_results)`. 
```{r summarise test results}
summary(test_results)
```

The results here are in a dataframe, showing the expression run, along with its min, mean, median, upper and lower quartiles, and the number of times it was ran. You  also get information about what unit the runtime was measured in. Here, it's microseconds.
In this case, the calculation is incredibly fast - the mean runtime was `summary(test_results)$mean` microseconds.  

If you're interested in more specifics of `microbenchmark()`, run `?microbenchmark` or `help("microbenchmark")` in your RStudio console. There are a number of optional arguments to the `microbenchmark()` function, including `times`, which specifies 'the number of times to evaluate the expression, and `unit`, the unit of measurement for the runtime. 

### using microbenchmark to compare code
`microbenchmark's` real power comes from its ability to compare different expressions in R. Let's see if the R function `mean()` is faster than computing it ourselves using the `sum()` and `length()` functions. 

```{r microbenchmark with mean()}
test_results_2 <- microbenchmark(
  'mean()'=mean(vec),
  'sum()&length()'=sum(vec)/length(vec)
)
summary(test_results_2)
```

Here, the results are in nanoseconds. It turns out that using `sum()` and `length()` is actually faster than `mean()`. However, with a difference as trivial as `r abs(summary(test_results_2)$mean[1]-summary(test_results_2)$mean[2])` nanoseconds, it's probably best to use the simple convenience function `mean()`.

### Plotting results
You can even plot the results quite easily, using the `autoplot()` function from `ggplot2`, which automatically draws the plot that works best with the class of object it is given (in this case, it makes a violin plot).

```{r autoplot results, message=F}
autoplot(test_results_2)
```

Calculating a mean of `r length(vec)` numbers is a computationally trivial thing to do. However, not all the code you write will be doing something so simple. Next, we will learn how for loops and vectorization compare, and then measure the differences using `microbenchmark`.
  
### Placeholder for exercise
### Exercise 1


```{r, echo=TRUE}
vector_1 <- c(5,4,3,1,6,4,3,2,6,4,2,1,5,6,7,3,2,9,5,8,9)
```

Above we created a numeric vector, called `vector_1`. Standardize the vector by subtracting the mean of `vector_1` from each score and then dividing by `vector_1`'s standard deviation. Then, use `microbenchmark()` to measure the computation time in \textbf{seconds}, and plot your results.

```{r xtabs, exercise=TRUE}
#standardize vector_1 and use microbenchmark() to measure the computation time.

```

```{r xtabs-solution, message=F}
vector_1 <- (vector_1-mean(vector_1))/sd(vector_1)
results <- microbenchmark(
  'standardize'= vector_1 <- (vector_1-mean(vector_1))/sd(vector_1),
  unit='s'
)
autoplot(results)
```


## **For loop review**

Let's start off with a review of for loops. To do so, we're going to use the `flights` dataset. 

```{r flights dataset, echo=F}
head(flights)
```

In this dataset, each row is an individual flight out of New York City. The column `dep_delay` contains the difference between the scheduled departure and the actual departure time, but in minutes. Now let's say we wanted to convert that to hours. To do so, we would divide each value in `dep_delay` by 60. We can do that using a `for` loop, and store the results in a vector.

First, we store all the departure delays in minutes, in a vector called `delays_mins`(making sure to remove `NA` values for when a delay was not available). The number of delays is calculated using the `length` function, and is stored as `n_delays`. Then, we create the vector `delays_hours_loop` to store the results of our computation, and the `mins_to_hours()` function to actually compute the results. 

```{r compute flight delays in hrs with for loop}
delays_mins <- !is.na(flights$dep_delay) #for simplicity, we remove NA values
n_delays <- length(delays_mins) #number of delays will equal the length of the vector
delays_hours_loop <- vector('numeric',n_delays)
mins_to_hours <- function(x) x/60
for(i in 1:n_delays){
  delays_hours_loop[i] <- mins_to_hours(delays_mins[i])
}
```

```{r show flight delays in hours}
head(delays_hours_loop)
```
The operation worked just fine, and the results are stored in `delays_hours_loop`.
However, given that there are `r n_delays` observations in this dataset, you may imagine that this many individual function calls can get computationally intensive. Though our current function is fairly simple, you may imagine that with more complex operations, the computation cost will add up fast. The next section will go through an alternative to for loops.
  
### Placeholder for exercise
<!-- add an exercise -->

## **Vectorization**
Rather than using a for loop, we can \textbf{vectorize} our conversion of minutes to hours function by applying it to all the elements in `delays_mins` simultaneously. This is called \textbf{vectorizing}. Doing so is fairly simple, and it's often faster than a for loop.

We can use our previous example of converting flight delays to hours.
```{r compute flight delays in hrs with vectorization}
delays_mins <- !is.na(flights$dep_time)
delays_hours <- delays_mins/60
```

Notice that here, we didn't even have to write a function. We just performed the operation by dividing  `delays_mins` by 60 (which automatically performs the division on each element of the vector, i.e. \textbf{vectorizing}). We could have called the `mins_to_hours()` function, but it's not necessary to do so.

### Comparing vectorization to loops

Now that we know about for loops and vectorization, we can use `microbenchmark` to compare the two. We will use the same example, converting `delays_mins` to hours, using both methods.

```{r message=F}
results_lv <- microbenchmark(
  'loop'=for(i in 1:n_delays){
  delays_hours_loop[i] <- mins_to_hours(delays_mins[i])
},
  'vectorize'=delays_hours_vectorize <- delays_mins/60
)
summary(results_lv)
autoplot(results_lv)
```
  
Even ignoring the obvious the advantage of being simpler to read, vectorization is clearly faster than a for loop in this case. 

### Placeholder for exercise
<!-- add an exercise -->

## **Vectorization using the apply family of functions**

Sometimes you have something more complicated, that can't be performed in a single line of code. You can do it in a for loop, or you can use a function from the `apply` family to vectorize it. 

To demonstrate the apply functions, let's start out with a common example: Reading in data files. 

It's common to have to read in multiple data files in social science research. You may have an individual data file for each participant, and you need to combine them all into a single data frame in R.

First, we'll use `list.files()` to store the names of all files in our current working directory to an object in R.

```{r get files in directory}
files <- list.files()
head(files)
```
 
As you can see, we have a number of data files in our directory, all stored as files with a `.csv` extension.

We can use a for loop to read in all the data files, and save each one as a dataframe inside a list.

```{r read in files with for loop, message=F}
data <- vector('list',length(files)) #initialize list
for(i in 1:length(files)){
  data[[i]]<-read_csv(files[[i]])
}
```

Finally, we can use the `dplyr` function `bind_rows()` to bind all the dataframes into one, larger dataframe.

```{r message=F}
dataset <- data %>%
  bind_rows()
glimpse(dataset)
```

\textbf{But wait!} There's a simpler (often faster) way to do it, using the `apply` family of functions. 

There are several `apply` functions in R, including `lapply()`, `sapply()`, `vapply()`. They all work by taking an argument `X`, which is a data structure (e.g., vector) containing all the elements to apply a function to. The function is specified with the argument `FUN`. 

### lapply()
There are some differences between the functions, so we'll start out with the most common version, `lapply()`. `lapply()` always returns data in the form of a list.

Below we use `lapply()` to use the `read_csv()` function on each element of `files`, storing the results in a list called `data1`. 

```{r message=F}
data1 <- lapply(files, read_csv) %>% 
  bind_rows()
glimpse(data1)
```


In the above code, lapply calls the `read_csv()` function once for each value `files`. Each time it calls `read_csv()`, it uses that particular value in `files` as input to the function. <!-- need better explanation --> 

Note that the results will be identical between the `for` loop and the `lapply` version.

However, the `lapply` version is both simpler to read and computationally faster. We can verify this using `microbenchmark`.

```{r message=F} 
files <-list.files()
result <- microbenchmark(
  'loop'=for(i in 1:length(files)){
    data[[i]]<-read_csv(files[[i]])
  },
  'lapply'=data1 <- lapply(files, read_csv),
  times = 50, # 'times' is used to specify the number of times to run each chunk of code
  unit='s' # 'unit' specifies the unit of measurement we ask for ('s'=seconds)
)
summary(result)
autoplot(result)
```

### Place holder for exercise

## **Other apply functions**

### lapply()

### vapply()

## **purrr**

### purr functions

### Placeholder for exercise

## **Conclusion & Glossary**

